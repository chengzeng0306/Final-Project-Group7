{
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from transformers import TFBertModel,BertTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,confusion_matrix,classification_report,f1_score\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler\n",
    "early_stoping = EarlyStopping(patience=3)\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "from transformers import BertModel,AdamWeightDecay, AdamW\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train,dev = train_test_split(train,test_size=0.2,random_state=2020)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train['target'].value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "sep = '[sep]'\n",
    "cls = '[cls]'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 32"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "#bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_model.save_pretrained('./save/')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "\n",
    "    cls_token = sequence_output[:,0,:]\n",
    "    # CNN layrers\n",
    "    out = Dense(3, activation='softmax')(cls_token)\n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        return_attention_masks=False,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n\n    return np.array(enc_di['input_ids'])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "bert = build_model(model,MAX_LENGTH)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "train_X = regular_encode(train['question_text'],tokenizer,MAX_LENGTH)\ndev_X = regular_encode(dev['question_text'],tokenizer,MAX_LENGTH)\ntest_X = regular_encode(test['question_text'],tokenizer,MAX_LENGTH)\n\ndev_label = to_categorical(dev['target'],num_classes=2)\ntrain_label = to_categorical(train['target'],num_classes=2)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "bert.fit(train_X,train_label,\n         batch_size=128,\n         validation_data=(dev_X,dev_label),\n         shuffle=True,epochs=EPOCHS,callbacks=[early_stoping])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "pred = bert.predict(test_X).argmax(-1)\nqid = test['qid']\ntest['prediction'] = list(pred)\ndf = test[['qid','prediction']]\ndf.to_csv('submission.csv',index=False)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}